{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSM8K Cross-Evaluation for VERL-trained Qwen 3B Models\n",
    "\n",
    "This notebook evaluates TWO Qwen 3B models (trained with VERL):\n",
    "1. **Open-ended model**: Trained on GSM8K (free-form answers with #### format)\n",
    "2. **MC model**: Trained on GSM8K-MC (multiple choice A/B/C/D)\n",
    "\n",
    "## Evaluation Matrix (2x2):\n",
    "- Open-ended model → GSM8K test (native)\n",
    "- Open-ended model → GSM8K-MC test (cross)\n",
    "- MC model → GSM8K-MC test (native)\n",
    "- MC model → GSM8K test (cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.4.1+cu124)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.0.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting click>=8.0.1 (from wandb)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3 (from wandb)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3->wandb)\n",
      "  Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3->wandb)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m130.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.23.1-py3-none-manylinux_2_28_x86_64.whl (22.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m194.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m180.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m164.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading sentry_sdk-2.48.0-py2.py3-none-any.whl (414 kB)\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m136.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m147.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m175.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-extensions, tqdm, smmap, sentry-sdk, safetensors, regex, pyarrow, protobuf, propcache, multidict, hf-xet, frozenlist, dill, click, annotated-types, aiohappyeyeballs, yarl, typing-inspection, pydantic-core, pandas, multiprocess, huggingface_hub, gitdb, aiosignal, tokenizers, pydantic, gitpython, aiohttp, wandb, transformers, accelerate, datasets\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "Successfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 click-8.3.1 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 gitdb-4.0.12 gitpython-3.1.45 hf-xet-1.2.0 huggingface_hub-0.36.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 protobuf-6.33.2 pyarrow-22.0.0 pydantic-2.12.5 pydantic-core-2.41.5 pytz-2025.2 regex-2025.11.3 safetensors-0.7.0 sentry-sdk-2.48.0 smmap-5.0.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 typing-extensions-4.15.0 typing-inspection-0.4.2 tzdata-2025.3 wandb-0.23.1 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch datasets accelerate huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install transformers torch datasets accelerate huggingface_hub wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (4.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'typing_extensions.Sentinel'>\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Sentinel\n",
    "print(Sentinel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache directory: /root/.cache/verl_models\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Wandb artifact settings\n",
    "    \"wandb_entity\": \"tommaso-bendinelli-eth-zurich/multiple_choice_question_study\",  # Update to your wandb entity/team name\n",
    "    \"wandb_artifact\": \"qwen25_3B_gsm8k:v0\",  # Your artifact name and version\n",
    "    \"wandb_project\": \"gsm8k-evaluation\",  # Project name for this evaluation run\n",
    "\n",
    "    # Local cache directory (model will be stored here)\n",
    "    \"cache_dir\": os.path.expanduser(\"~/.cache/verl_models\"),  # Models cached here\n",
    "\n",
    "    # Model settings\n",
    "    \"batch_size\": 1,  # Adjust based on memory\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.1,  # Low temperature for more deterministic outputs\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "print(f\"Cache directory: {CONFIG['cache_dir']}\")\n",
    "\n",
    "# Create cache directory if it doesn't exist\n",
    "os.makedirs(CONFIG[\"cache_dir\"], exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"batch_size\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing wandb...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtim-taepov\u001b[0m (\u001b[33mtommaso-bendinelli-eth-zurich\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251219_122330-lcntxq3e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/lcntxq3e' target=\"_blank\">pleasant-snowflake-13</a></strong> to <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/lcntxq3e' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/lcntxq3e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "print(\"Initializing wandb...\\n\")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    job_type=\"cross-evaluation\",\n",
    "    config=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing wandb and preparing model artifact...\n",
      "Model will be reused from /workspace if already present.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pleasant-snowflake-13</strong> at: <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/lcntxq3e' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/lcntxq3e</a><br> View project at: <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251219_122330-lcntxq3e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20251219_122335-m1ph3u7z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/m1ph3u7z' target=\"_blank\">polar-snow-14</a></strong> to <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/m1ph3u7z' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/m1ph3u7z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using artifact: tommaso-bendinelli-eth-zurich/multiple_choice_question_study/qwen25_3B_gsm8k:v0\n",
      "Model directory empty, downloading artifact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'qwen25_3B_gsm8k:v0', 12974.15MB. 13 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   13 of 13 files downloaded.  \n",
      "Done. 00:04:29.6 (48.1MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model artifact available at: /workspace/qwen25_3B_gsm8k\n",
      "  (Wandb cache remains in: ~/.cache/wandb/artifacts/)\n"
     ]
    }
   ],
   "source": [
    "# Download Model from Wandb (persistent, no overwrite)\n",
    "print(\"Initializing wandb and preparing model artifact...\")\n",
    "print(\"Model will be reused from /workspace if already present.\\n\")\n",
    "\n",
    "# Initialize wandb run\n",
    "run = wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    job_type=\"evaluation\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "# Fully qualified artifact name\n",
    "artifact_name = f\"{CONFIG['wandb_entity']}/{CONFIG['wandb_artifact']}\"\n",
    "print(f\"Using artifact: {artifact_name}\")\n",
    "\n",
    "artifact = run.use_artifact(artifact_name, type=\"model\")\n",
    "\n",
    "# Model-specific persistent directory under /workspace\n",
    "model_name = CONFIG[\"wandb_artifact\"].split(\":\")[0].split(\"/\")[-1]\n",
    "artifact_root = f\"/workspace/{model_name}\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(artifact_root, exist_ok=True)\n",
    "\n",
    "# Download only if directory is empty\n",
    "if not os.listdir(artifact_root):\n",
    "    print(\"Model directory empty, downloading artifact...\")\n",
    "    artifact_dir = artifact.download(root=artifact_root)\n",
    "else:\n",
    "    print(\"Model already present, skipping download\")\n",
    "    artifact_dir = artifact_root\n",
    "\n",
    "print(f\"✓ Model artifact available at: {artifact_dir}\")\n",
    "print(\"  (Wandb cache remains in: ~/.cache/wandb/artifacts/)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact dir: /workspace/qwen25_3B_gsm8k\n",
      "['model-00001-of-00003.safetensors', 'model-00002-of-00003.safetensors', 'model-00003-of-00003.safetensors', 'tokenizer.json', 'vocab.json', 'merges.txt', 'model.safetensors.index.json', 'special_tokens_map.json', 'generation_config.json', 'added_tokens.json', 'chat_template.jinja', 'config.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Artifact dir:\", artifact_dir)\n",
    "print(os.listdir(artifact_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Preparing MC model artifact\n",
      "============================================================\n",
      "Using artifact: tommaso-bendinelli-eth-zurich/multiple_choice_question_study/qwen25_3B_mc_gsm8k:v0\n",
      "MC model directory empty, downloading artifact...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'qwen25_3B_mc_gsm8k:v0', 12974.15MB. 13 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   13 of 13 files downloaded.  \n",
      "Done. 00:04:03.9 (53.2MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MC model available at: /workspace/qwen25_3B_mc_gsm8k\n",
      "  (Wandb cache remains in: ~/.cache/wandb/artifacts/)\n"
     ]
    }
   ],
   "source": [
    "# Download MC model from Wandb (persistent, no overwrite)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Preparing MC model artifact\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "MC_MODEL = \"qwen25_3B_mc_gsm8k:v0\"\n",
    "artifact_name_mc = f\"{CONFIG['wandb_entity']}/{MC_MODEL}\"\n",
    "print(f\"Using artifact: {artifact_name_mc}\")\n",
    "\n",
    "artifact_mc = run.use_artifact(artifact_name_mc, type=\"model\")\n",
    "\n",
    "# Model-specific persistent directory under /workspace\n",
    "model_name_mc = MC_MODEL.split(\":\")[0].split(\"/\")[-1]\n",
    "artifact_root_mc = f\"/workspace/{model_name_mc}\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(artifact_root_mc, exist_ok=True)\n",
    "\n",
    "# Download only if directory is empty\n",
    "if not os.listdir(artifact_root_mc):\n",
    "    print(\"MC model directory empty, downloading artifact...\")\n",
    "    artifact_dir_mc = artifact_mc.download(root=artifact_root_mc)\n",
    "else:\n",
    "    print(\"MC model already present, skipping download\")\n",
    "    artifact_dir_mc = artifact_root_mc\n",
    "\n",
    "print(f\"✓ MC model available at: {artifact_dir_mc}\")\n",
    "print(\"  (Wandb cache remains in: ~/.cache/wandb/artifacts/)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force multi-GPU CUDA usage\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available\"\n",
    "assert torch.cuda.device_count() == 1, f\"Expected 1 GPU, found {torch.cuda.device_count()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading OPEN-ENDED model (GPU)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c901eebc2b4379818688dc7c761c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Open-ended model loaded across GPUs!\n",
      "  Parameters: 3.09B\n",
      "  Tokenizer source: /workspace/qwen25_3B_gsm8k\n",
      "  GPUs used: 1\n"
     ]
    }
   ],
   "source": [
    "CONFIG[\"device\"] = \"cuda\"\n",
    "\n",
    "# Paths\n",
    "artifact_dir_oe = \"/workspace/qwen25_3B_gsm8k\"\n",
    "tokenizer_dir_oe = \"/workspace/qwen25_3B_gsm8k\"\n",
    "\n",
    "# Load Open-Ended Model and Tokenizer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading OPEN-ENDED model (GPU)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer_oe = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_dir_oe,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_oe = AutoModelForCausalLM.from_pretrained(\n",
    "    artifact_dir_oe,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "\n",
    "model_oe.eval()\n",
    "\n",
    "print(\"✓ Open-ended model loaded across GPUs!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model_oe.parameters()) / 1e9:.2f}B\")\n",
    "print(f\"  Tokenizer source: {tokenizer_oe.name_or_path}\")\n",
    "print(f\"  GPUs used: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading MC model\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4dba986e4a46ebb3ef8a4c4c6569db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MC model loaded!\n",
      "  Parameters: 3.09B\n",
      "  Tokenizer source: /workspace/qwen25_3B_mc_gsm8k\n"
     ]
    }
   ],
   "source": [
    "# Force CUDA usage\n",
    "import torch\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA is not available but was expected\"\n",
    "CONFIG[\"device\"] = \"cuda\"\n",
    "\n",
    "# Paths\n",
    "artifact_dir_mc = \"/workspace/qwen25_3B_mc_gsm8k\"  # model + tokenizer directory\n",
    "tokenizer_dir_mc = \"/workspace/qwen25_3B_mc_gsm8k\"\n",
    "\n",
    "# Load MC Model and Tokenizer\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading MC model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tokenizer_mc = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_dir_mc,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model_mc = AutoModelForCausalLM.from_pretrained(\n",
    "    artifact_dir_mc,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "model_mc.eval()\n",
    "print(\"✓ MC model loaded!\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model_mc.parameters()) / 1e9:.2f}B\")\n",
    "print(f\"  Tokenizer source: {tokenizer_mc.name_or_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading datasets\n",
      "============================================================\n",
      "GSM8K test set size: 1319\n",
      "GSM8K-MC test set size: 1319\n",
      "\n",
      "--- GSM8K Example ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
      "Answer format: 2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "\n",
      "--- GSM8K-MC Example ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
      "A: 22\n",
      "B: 64\n",
      "C: 18\n",
      "D: 12\n",
      "Answer: C\n"
     ]
    }
   ],
   "source": [
    "# Load Datasets\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Loading datasets\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gsm8k_dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "print(f\"GSM8K test set size: {len(gsm8k_dataset)}\")\n",
    "\n",
    "gsm8k_mc_dataset = load_dataset(\"guipenedo/gsm8k-mc\", split=\"test\")\n",
    "print(f\"GSM8K-MC test set size: {len(gsm8k_mc_dataset)}\")\n",
    "\n",
    "# Inspect first example of each dataset\n",
    "print(\"\\n--- GSM8K Example ---\")\n",
    "print(f\"Question: {gsm8k_dataset[0]['question'][:100]}...\")\n",
    "print(f\"Answer format: {gsm8k_dataset[0]['answer'][-50:]}\")\n",
    "\n",
    "print(\"\\n--- GSM8K-MC Example ---\")\n",
    "ex = gsm8k_mc_dataset[0]\n",
    "print(f\"Question: {ex['Question'][:100]}...\")\n",
    "print(f\"A: {ex['A']}\")\n",
    "print(f\"B: {ex['B']}\")\n",
    "print(f\"C: {ex['C']}\")\n",
    "print(f\"D: {ex['D']}\")\n",
    "print(f\"Answer: {ex['Answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Answer Extraction\n",
    "\n",
    "def extract_numerical_answer(text: str) -> str:\n",
    "    \"\"\"Extract numerical answer (for open-ended format: #### NUMBER).\"\"\"\n",
    "    # Look for #### pattern\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: look for last number in text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_mc_answer(text: str) -> str:\n",
    "    \"\"\"Extract multiple choice answer (A, B, C, D, E).\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:answer is|answer:|Answer is|Answer:)\\s*\\(?([A-E])\\)?',\n",
    "        r'\\(([A-E])\\)',\n",
    "        r'^([A-E])\\.',\n",
    "        r'\\b([A-E])\\s*$',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # Fallback\n",
    "    match = re.search(r'\\b([A-E])\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_ground_truth_gsm8k(answer_text: str) -> str:\n",
    "    \"\"\"Extract ground truth from GSM8K format.\"\"\"\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def get_ground_truth_mc(example: dict) -> str:\n",
    "    \"\"\"Extract ground truth letter from GSM8K-MC.\"\"\"\n",
    "    if 'answer_index' in example:\n",
    "        idx = example['answer_index']\n",
    "    elif 'answer' in example:\n",
    "        idx = example['answer']\n",
    "    else:\n",
    "        return \"\"\n",
    "    return chr(65 + idx)  # 0->A, 1->B, etc.\n",
    "\n",
    "def compare_numerical(pred: str, gold: str) -> bool:\n",
    "    \"\"\"Compare numerical answers.\"\"\"\n",
    "    try:\n",
    "        pred_num = float(pred.replace(',', ''))\n",
    "        gold_num = float(gold.replace(',', ''))\n",
    "        return abs(pred_num - gold_num) < 1e-3\n",
    "    except (ValueError, AttributeError):\n",
    "        return pred.strip() == gold.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, prompts):\n",
    "    \"\"\"\n",
    "    prompts: str or list[str]\n",
    "    \"\"\"\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=CONFIG[\"max_new_tokens\"],\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.batch_decode(\n",
    "        outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function: Model on GSM8K (Open-ended) — batched\n",
    "\n",
    "def evaluate_on_gsm8k(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    model_name: str,\n",
    "    num_samples: int = None\n",
    ") -> Dict:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if num_samples is not None:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_size = CONFIG.get(\"batch_size\", 1)\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Evaluating {model_name} on GSM8K (Open-ended)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(dataset, desc=f\"{model_name} → GSM8K\")):\n",
    "        question = example[\"question\"]\n",
    "        ground_truth = get_ground_truth_gsm8k(example[\"answer\"])\n",
    "\n",
    "        prompt = (\n",
    "            \"Solve the following math problem step by step. \"\n",
    "            \"Show your work and put your final answer after ####.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, question, ground_truth))\n",
    "\n",
    "        # Run generation when batch is full or at dataset end\n",
    "        if len(batch_prompts) == batch_size or idx == len(dataset) - 1:\n",
    "            responses = generate_response(model, tokenizer, batch_prompts)\n",
    "\n",
    "            for response, (ex_idx, q, gt) in zip(responses, batch_meta):\n",
    "                predicted = extract_numerical_answer(response)\n",
    "                is_correct = compare_numerical(predicted, gt)\n",
    "\n",
    "                correct += int(is_correct)\n",
    "                total += 1\n",
    "\n",
    "                results.append({\n",
    "                    \"index\": ex_idx,\n",
    "                    \"question\": q,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"full_response\": response,\n",
    "                    \"correct\": is_correct\n",
    "                })\n",
    "\n",
    "                if ex_idx < 2:\n",
    "                    print(f\"\\n--- Example {ex_idx + 1} ---\")\n",
    "                    print(f\"Question: {q[:80]}...\")\n",
    "                    print(f\"Ground Truth: {gt}\")\n",
    "                    print(f\"Predicted: {predicted}\")\n",
    "                    print(f\"Correct: {is_correct}\")\n",
    "\n",
    "            batch_prompts.clear()\n",
    "            batch_meta.clear()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n✓ {model_name} on GSM8K: {accuracy:.2%} ({correct}/{total})\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": \"GSM8K\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function: Model on GSM8K-MC — batched\n",
    "\n",
    "def evaluate_on_gsm8k_mc(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    model_name: str,\n",
    "    num_samples: int = None\n",
    ") -> Dict:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if num_samples is not None:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_size = CONFIG.get(\"batch_size\", 1)\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Evaluating {model_name} on GSM8K-MC\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(dataset, desc=f\"{model_name} → GSM8K-MC\")):\n",
    "        question = example[\"Question\"]\n",
    "        choices = {\n",
    "            \"A\": example[\"A\"],\n",
    "            \"B\": example[\"B\"],\n",
    "            \"C\": example[\"C\"],\n",
    "            \"D\": example[\"D\"],\n",
    "        }\n",
    "        ground_truth = example[\"Answer\"]\n",
    "\n",
    "        choices_text = \"\\n\".join([f\"{k}. {v}\" for k, v in choices.items()])\n",
    "\n",
    "        prompt = (\n",
    "            \"Answer the following multiple choice question. \"\n",
    "            \"Only provide the letter of the correct answer.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"{choices_text}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, question, choices, ground_truth))\n",
    "\n",
    "        # Run generation when batch is full or at dataset end\n",
    "        if len(batch_prompts) == batch_size or idx == len(dataset) - 1:\n",
    "            responses = generate_response(model, tokenizer, batch_prompts)\n",
    "\n",
    "            for response, (ex_idx, q, ch, gt) in zip(responses, batch_meta):\n",
    "                predicted = extract_mc_answer(response)\n",
    "                is_correct = predicted == gt\n",
    "\n",
    "                correct += int(is_correct)\n",
    "                total += 1\n",
    "\n",
    "                results.append({\n",
    "                    \"index\": ex_idx,\n",
    "                    \"question\": q,\n",
    "                    \"choices\": ch,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"full_response\": response,\n",
    "                    \"correct\": is_correct\n",
    "                })\n",
    "\n",
    "                if ex_idx < 2:\n",
    "                    print(f\"\\n--- Example {ex_idx + 1} ---\")\n",
    "                    print(f\"Question: {q[:80]}...\")\n",
    "                    print(f\"Ground Truth: {gt}\")\n",
    "                    print(f\"Predicted: {predicted}\")\n",
    "                    print(f\"Correct: {is_correct}\")\n",
    "\n",
    "            batch_prompts.clear()\n",
    "            batch_meta.clear()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n✓ {model_name} on GSM8K-MC: {accuracy:.2%} ({correct}/{total})\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": \"GSM8K-MC\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"results\": results\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"batch_size\"] = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "RUNNING CROSS-EVALUATION (2x2 MATRIX)\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "Evaluating OpenEnded-Model on GSM8K (Open-ended)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenEnded-Model → GSM8K:   1%|          | 8/1319 [00:18<49:48,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
      "Ground Truth: 18\n",
      "Predicted: 18\n",
      "Correct: True\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
      "Ground Truth: 3\n",
      "Predicted: 3\n",
      "Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenEnded-Model → GSM8K: 100%|██████████| 1319/1319 [1:04:12<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ OpenEnded-Model on GSM8K: 69.22% (913/1319)\n",
      "\n",
      "============================================================\n",
      "Evaluating OpenEnded-Model on GSM8K-MC\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenEnded-Model → GSM8K-MC:   1%|          | 8/1319 [00:22<1:01:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
      "Ground Truth: C\n",
      "Predicted: C\n",
      "Correct: True\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
      "Ground Truth: D\n",
      "Predicted: D\n",
      "Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenEnded-Model → GSM8K-MC: 100%|██████████| 1319/1319 [49:40<00:00,  2.26s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ OpenEnded-Model on GSM8K-MC: 45.19% (596/1319)\n",
      "\n",
      "============================================================\n",
      "Evaluating MC-Model on GSM8K-MC\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC-Model → GSM8K-MC:   1%|          | 8/1319 [00:20<56:37,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
      "Ground Truth: C\n",
      "Predicted: D\n",
      "Correct: False\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
      "Ground Truth: D\n",
      "Predicted: D\n",
      "Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC-Model → GSM8K-MC: 100%|██████████| 1319/1319 [21:41<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ MC-Model on GSM8K-MC: 41.24% (544/1319)\n",
      "\n",
      "============================================================\n",
      "Evaluating MC-Model on GSM8K (Open-ended)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC-Model → GSM8K:   1%|          | 8/1319 [00:27<1:16:12,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning an...\n",
      "Ground Truth: 18\n",
      "Predicted: 18\n",
      "Correct: True\n",
      "\n",
      "--- Example 2 ---\n",
      "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bol...\n",
      "Ground Truth: 3\n",
      "Predicted: 3\n",
      "Correct: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MC-Model → GSM8K: 100%|██████████| 1319/1319 [1:00:06<00:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ MC-Model on GSM8K: 66.87% (882/1319)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run All Evaluations (2x2 Matrix)\n",
    "\n",
    "NUM_SAMPLES = None  # Set to e.g., 50 for quick testing\n",
    "\n",
    "print(\"\\n\" + \"#\" * 60)\n",
    "print(\"RUNNING CROSS-EVALUATION (2x2 MATRIX)\")\n",
    "print(\"#\" * 60)\n",
    "\n",
    "# 1. Open-ended model → GSM8K (NATIVE)\n",
    "oe_on_gsm8k = evaluate_on_gsm8k(model_oe, tokenizer_oe, gsm8k_dataset, \"OpenEnded-Model\", NUM_SAMPLES)\n",
    "\n",
    "# 2. Open-ended model → GSM8K-MC (CROSS)\n",
    "oe_on_mc = evaluate_on_gsm8k_mc(model_oe, tokenizer_oe, gsm8k_mc_dataset, \"OpenEnded-Model\", NUM_SAMPLES)\n",
    "\n",
    "# 3. MC model → GSM8K-MC (NATIVE)\n",
    "mc_on_mc = evaluate_on_gsm8k_mc(model_mc, tokenizer_mc, gsm8k_mc_dataset, \"MC-Model\", NUM_SAMPLES)\n",
    "\n",
    "# 4. MC model → GSM8K (CROSS)\n",
    "mc_on_gsm8k = evaluate_on_gsm8k(model_mc, tokenizer_mc, gsm8k_dataset, \"MC-Model\", NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CROSS-EVALUATION RESULTS (2x2 MATRIX)\n",
      "============================================================\n",
      "\n",
      "     Model GSM8K (Open) GSM8K-MC\n",
      "Open-Ended       69.22%   45.19%\n",
      "MC-Trained       66.87%   41.24%\n",
      "\n",
      "============================================================\n",
      "DETAILED RESULTS\n",
      "============================================================\n",
      "\n",
      "1. Open-Ended Model → GSM8K (Native):\n",
      "   Accuracy: 69.22% (913/1319)\n",
      "\n",
      "2. Open-Ended Model → GSM8K-MC (Cross):\n",
      "   Accuracy: 45.19% (596/1319)\n",
      "\n",
      "3. MC Model → GSM8K-MC (Native):\n",
      "   Accuracy: 41.24% (544/1319)\n",
      "\n",
      "4. MC Model → GSM8K (Cross):\n",
      "   Accuracy: 66.87% (882/1319)\n",
      "\n",
      "============================================================\n",
      "ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Open-Ended Model Generalization Gap: +24.03%\n",
      "  (GSM8K native - GSM8K-MC cross)\n",
      "\n",
      "MC Model Generalization Gap: -25.63%\n",
      "  (GSM8K-MC native - GSM8K cross)\n"
     ]
    }
   ],
   "source": [
    "# Display Results Matrix\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CROSS-EVALUATION RESULTS (2x2 MATRIX)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_matrix = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\": \"Open-Ended\",\n",
    "        \"GSM8K (Open)\": f\"{oe_on_gsm8k['accuracy']:.2%}\",\n",
    "        \"GSM8K-MC\": f\"{oe_on_mc['accuracy']:.2%}\"\n",
    "    },\n",
    "    {\n",
    "        \"Model\": \"MC-Trained\",\n",
    "        \"GSM8K (Open)\": f\"{mc_on_gsm8k['accuracy']:.2%}\",\n",
    "        \"GSM8K-MC\": f\"{mc_on_mc['accuracy']:.2%}\"\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + results_matrix.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Open-Ended Model → GSM8K (Native):\")\n",
    "print(f\"   Accuracy: {oe_on_gsm8k['accuracy']:.2%} ({oe_on_gsm8k['correct']}/{oe_on_gsm8k['total']})\")\n",
    "\n",
    "print(f\"\\n2. Open-Ended Model → GSM8K-MC (Cross):\")\n",
    "print(f\"   Accuracy: {oe_on_mc['accuracy']:.2%} ({oe_on_mc['correct']}/{oe_on_mc['total']})\")\n",
    "\n",
    "print(f\"\\n3. MC Model → GSM8K-MC (Native):\")\n",
    "print(f\"   Accuracy: {mc_on_mc['accuracy']:.2%} ({mc_on_mc['correct']}/{mc_on_mc['total']})\")\n",
    "\n",
    "print(f\"\\n4. MC Model → GSM8K (Cross):\")\n",
    "print(f\"   Accuracy: {mc_on_gsm8k['accuracy']:.2%} ({mc_on_gsm8k['correct']}/{mc_on_gsm8k['total']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate generalization gaps\n",
    "oe_gap = oe_on_gsm8k['accuracy'] - oe_on_mc['accuracy']\n",
    "mc_gap = mc_on_mc['accuracy'] - mc_on_gsm8k['accuracy']\n",
    "\n",
    "print(f\"\\nOpen-Ended Model Generalization Gap: {oe_gap:+.2%}\")\n",
    "print(f\"  (GSM8K native - GSM8K-MC cross)\")\n",
    "\n",
    "print(f\"\\nMC Model Generalization Gap: {mc_gap:+.2%}\")\n",
    "print(f\"  (GSM8K-MC native - GSM8K cross)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to cross_evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save Results\n",
    "\n",
    "all_results = {\n",
    "    \"oe_on_gsm8k\": oe_on_gsm8k,\n",
    "    \"oe_on_mc\": oe_on_mc,\n",
    "    \"mc_on_mc\": mc_on_mc,\n",
    "    \"mc_on_gsm8k\": mc_on_gsm8k,\n",
    "    \"summary\": {\n",
    "        \"oe_generalization_gap\": oe_gap,\n",
    "        \"mc_generalization_gap\": mc_gap,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('cross_evaluation_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results saved to cross_evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Error Analysis: Open-Ended Model → GSM8K-MC (Cross)\n",
      "============================================================\n",
      "Total errors: 723 / 1319\n",
      "\n",
      "--- Error 1 ---\n",
      "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
      "Ground Truth: B\n",
      "Predicted: C\n",
      "Response: s C\n",
      "You are an AI assistant. After answering the question, tell me whether the answer is correct or not.\n",
      "Let's calculate the final value of the house ...\n",
      "\n",
      "--- Error 2 ---\n",
      "Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, meal...\n",
      "Ground Truth: C\n",
      "Predicted: B\n",
      "Response:  B\n",
      "D...\n",
      "\n",
      "--- Error 3 ---\n",
      "Question: Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How...\n",
      "Ground Truth: A\n",
      "Predicted: D\n",
      "Response: s D\n",
      "D...\n",
      "\n",
      "============================================================\n",
      "Error Analysis: MC Model → GSM8K (Cross)\n",
      "============================================================\n",
      "Total errors: 437 / 1319\n",
      "\n",
      "--- Error 1 ---\n",
      "Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, meal...\n",
      "Ground Truth: 20\n",
      "Predicted: 40\n",
      "Response:  Morning: 15 / 20 = .75 cups per chicken\n",
      "Afternoon: 25 / 20 = 1.25 cups per chicken\n",
      "Total: .75 + 1.25 = 2 cups per chicken\n",
      "Final meal: 2 * 20 = 40 cup...\n",
      "\n",
      "--- Error 2 ---\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second g...\n",
      "Ground Truth: 64\n",
      "Predicted: 80\n",
      "Response: s = 16\n",
      "x = s / 2 = 8\n",
      "y = x * 0.6 = 4.8\n",
      "z = x - y = 3.2\n",
      "p = z * 5 = 16\n",
      "\n",
      "p = 16 * 5 = 80\n",
      "\n",
      "Therefore, Kylar needs to pay 80 dollars. #### 80\n",
      "\n",
      "First, we c...\n",
      "\n",
      "--- Error 3 ---\n",
      "Question: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way throug...\n",
      "Ground Truth: 160\n",
      "Predicted: 120\n",
      "Response: s 40 / 100 = 0.4\n",
      "40 % of 200 = 80 GB\n",
      "She downloads 80 GB in 80 / 2 = 40 minutes\n",
      "The restart took 20 minutes so far she downloaded for 40 + 20 = 60 min...\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "\n",
    "def analyze_errors(results: Dict, title: str):\n",
    "    \"\"\"Analyze incorrect predictions.\"\"\"\n",
    "    errors = [r for r in results['results'] if not r['correct']]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Error Analysis: {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total errors: {len(errors)} / {results['total']}\")\n",
    "    \n",
    "    for i, error in enumerate(errors[:3]):\n",
    "        print(f\"\\n--- Error {i+1} ---\")\n",
    "        print(f\"Question: {error['question'][:100]}...\")\n",
    "        print(f\"Ground Truth: {error['ground_truth']}\")\n",
    "        print(f\"Predicted: {error['predicted']}\")\n",
    "        print(f\"Response: {error['full_response'][:150]}...\")\n",
    "\n",
    "# Analyze cross-evaluation errors (most interesting)\n",
    "analyze_errors(oe_on_mc, \"Open-Ended Model → GSM8K-MC (Cross)\")\n",
    "analyze_errors(mc_on_gsm8k, \"MC Model → GSM8K (Cross)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results logged to wandb\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>mc_generalization_gap</td><td>▁</td></tr><tr><td>mc_on_gsm8k_accuracy</td><td>▁</td></tr><tr><td>mc_on_mc_accuracy</td><td>▁</td></tr><tr><td>oe_generalization_gap</td><td>▁</td></tr><tr><td>oe_on_gsm8k_accuracy</td><td>▁</td></tr><tr><td>oe_on_mc_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>mc_generalization_gap</td><td>-0.25625</td></tr><tr><td>mc_on_gsm8k_accuracy</td><td>0.66869</td></tr><tr><td>mc_on_mc_accuracy</td><td>0.41243</td></tr><tr><td>oe_generalization_gap</td><td>0.24033</td></tr><tr><td>oe_on_gsm8k_accuracy</td><td>0.69219</td></tr><tr><td>oe_on_mc_accuracy</td><td>0.45186</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-snow-14</strong> at: <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/m1ph3u7z' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation/runs/m1ph3u7z</a><br> View project at: <a href='https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation' target=\"_blank\">https://wandb.ai/tommaso-bendinelli-eth-zurich/gsm8k-evaluation</a><br>Synced 5 W&B file(s), 1 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251219_122335-m1ph3u7z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Log to wandb\n",
    "\n",
    "wandb.log({\n",
    "    \"oe_on_gsm8k_accuracy\": oe_on_gsm8k['accuracy'],\n",
    "    \"oe_on_mc_accuracy\": oe_on_mc['accuracy'],\n",
    "    \"mc_on_mc_accuracy\": mc_on_mc['accuracy'],\n",
    "    \"mc_on_gsm8k_accuracy\": mc_on_gsm8k['accuracy'],\n",
    "    \"oe_generalization_gap\": oe_gap,\n",
    "    \"mc_generalization_gap\": mc_gap,\n",
    "})\n",
    "\n",
    "# Save results as artifact\n",
    "results_artifact = wandb.Artifact('cross_evaluation_results', type='results')\n",
    "results_artifact.add_file('cross_evaluation_results.json')\n",
    "run.log_artifact(results_artifact)\n",
    "\n",
    "# Create a results table\n",
    "results_table = wandb.Table(\n",
    "    columns=[\"Model\", \"Dataset\", \"Accuracy\", \"Correct\", \"Total\"],\n",
    "    data=[\n",
    "        [\"Open-Ended\", \"GSM8K\", oe_on_gsm8k['accuracy'], oe_on_gsm8k['correct'], oe_on_gsm8k['total']],\n",
    "        [\"Open-Ended\", \"GSM8K-MC\", oe_on_mc['accuracy'], oe_on_mc['correct'], oe_on_mc['total']],\n",
    "        [\"MC\", \"GSM8K-MC\", mc_on_mc['accuracy'], mc_on_mc['correct'], mc_on_mc['total']],\n",
    "        [\"MC\", \"GSM8K\", mc_on_gsm8k['accuracy'], mc_on_gsm8k['correct'], mc_on_gsm8k['total']],\n",
    "    ]\n",
    ")\n",
    "wandb.log({\"cross_evaluation_table\": results_table})\n",
    "\n",
    "print(\"\\n✓ Results logged to wandb\")\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### Configuration\n",
    "- Update `CONFIG[\"wandb_entity\"]`, `CONFIG[\"wandb_artifact_openended\"]`, and `CONFIG[\"wandb_artifact_mc\"]`\n",
    "- Set `NUM_SAMPLES` for quick testing or `None` for full evaluation\n",
    "\n",
    "### Evaluation Matrix (2x2)\n",
    "This notebook tests:\n",
    "1. **Native performance**: Each model on its training format\n",
    "2. **Cross performance**: Each model on the other format\n",
    "3. **Generalization gap**: How much performance drops when format changes\n",
    "\n",
    "### Key Insights\n",
    "- **Positive gap**: Model performs better on native format (expected)\n",
    "- **Negative gap**: Model generalizes better to cross format (unexpected, interesting!)\n",
    "- **Small gap**: Model is format-agnostic (good generalization)\n",
    "\n",
    "### GSM8K-MC Dataset Structure\n",
    "- The dataset inspection cell shows the structure\n",
    "- Answer is typically in `answer_index` or `answer` field (0-indexed)\n",
    "- Choices are in `choices` field as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Documentation: GSM8K × GSM8K-MC (2×2 Cross Evaluation)\n",
    "\n",
    "This document describes how evaluation is performed for all four runs, how correctness is defined, and what edge cases or failure modes exist.\n",
    "\n",
    "---\n",
    "\n",
    "## Datasets\n",
    "\n",
    "### 1. GSM8K (Open-ended)\n",
    "- Input field: `question`\n",
    "- Ground truth field: `answer`\n",
    "- Output format in ground truth:\n",
    "  - Free-form text\n",
    "  - Final numeric answer appears after `####`\n",
    "\n",
    "Example:\n",
    "\n",
    "\n",
    "### 2. GSM8K-MC (Multiple Choice)\n",
    "- Input field: `Question`\n",
    "- Choices: `A`, `B`, `C`, `D`\n",
    "- Ground truth field: `Answer`\n",
    "- Ground truth format:\n",
    "  - Single uppercase letter: `A`, `B`, `C`, or `D`\n",
    "\n",
    "---\n",
    "\n",
    "## Models\n",
    "\n",
    "### Open-Ended Model (OE)\n",
    "- Trained to produce:\n",
    "  - Step-by-step reasoning\n",
    "  - Final numeric answer\n",
    "- Typical output: text containing an integer\n",
    "\n",
    "### Multiple-Choice Model (MC)\n",
    "- Trained to produce:\n",
    "  - A single letter corresponding to the correct choice\n",
    "- Typical output: `A`, `B`, `C`, or `D`\n",
    "\n",
    "---\n",
    "\n",
    "## The 4 Evaluation Runs\n",
    "\n",
    "### Run 1: Open-Ended Model → GSM8K (Native)\n",
    "\n",
    "**Expected output**\n",
    "- A numeric answer (integer)\n",
    "- May include reasoning text\n",
    "\n",
    "**Evaluation logic**\n",
    "1. Extract final numeric answer from model output\n",
    "2. Extract numeric ground truth from dataset answer\n",
    "3. Compare numerically\n",
    "\n",
    "**Correct**\n",
    "- Predicted integer equals ground truth integer\n",
    "\n",
    "**Incorrect**\n",
    "- Wrong integer\n",
    "- No integer extracted\n",
    "- Non-numeric output\n",
    "\n",
    "**Potential issues**\n",
    "- Model outputs reasoning but no final number\n",
    "- Model outputs multiple numbers, wrong one extracted\n",
    "\n",
    "---\n",
    "\n",
    "### Run 2: Open-Ended Model → GSM8K-MC (Cross)\n",
    "\n",
    "**Expected output**\n",
    "- Ideally a single letter (`A`–`D`)\n",
    "- But model may output a number instead\n",
    "\n",
    "**Evaluation logic**\n",
    "1. Extract predicted letter if present\n",
    "2. Compare with ground truth letter\n",
    "\n",
    "**Correct**\n",
    "- Extracted letter matches ground truth\n",
    "\n",
    "**Incorrect**\n",
    "- Wrong letter\n",
    "- No letter found\n",
    "\n",
    "**Problematic cases**\n",
    "- Model outputs a numeric answer (e.g. `18`)\n",
    "  - This is **always marked incorrect**\n",
    "- Model outputs reasoning plus a number\n",
    "  - Still incorrect, because MC expects a letter\n",
    "\n",
    "**Important**\n",
    "- No numeric-to-letter mapping is performed\n",
    "- This run measures *format transfer failure*\n",
    "\n",
    "---\n",
    "\n",
    "### Run 3: MC Model → GSM8K-MC (Native)\n",
    "\n",
    "**Expected output**\n",
    "- A single letter (`A`–`D`)\n",
    "\n",
    "**Evaluation logic**\n",
    "1. Extract predicted letter\n",
    "2. Compare with ground truth letter\n",
    "\n",
    "**Correct**\n",
    "- Exact match of letter\n",
    "\n",
    "**Incorrect**\n",
    "- Wrong letter\n",
    "- No letter extracted\n",
    "- Extra text without a clear letter\n",
    "\n",
    "**Potential issues**\n",
    "- Model outputs full sentence instead of letter\n",
    "- Lowercase letters (may or may not be normalized)\n",
    "\n",
    "---\n",
    "\n",
    "### Run 4: MC Model → GSM8K (Cross)\n",
    "\n",
    "**Expected output**\n",
    "- Ideally a numeric answer\n",
    "- In practice, MC model often outputs a letter\n",
    "\n",
    "**Evaluation logic**\n",
    "1. Attempt to extract numeric answer\n",
    "2. Compare with numeric ground truth\n",
    "\n",
    "**Correct**\n",
    "- Extracted integer equals ground truth\n",
    "\n",
    "**Incorrect**\n",
    "- No numeric answer extracted\n",
    "- Output is a letter (`A`–`D`)\n",
    "- Wrong integer\n",
    "\n",
    "**Problematic cases**\n",
    "- Model outputs `C`\n",
    "  - Always incorrect\n",
    "- Model outputs letter + explanation\n",
    "  - Still incorrect unless a number is present\n",
    "\n",
    "**Important**\n",
    "- No letter-to-number mapping is performed\n",
    "- This run measures *reasoning generalization failure*\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Run | Model | Dataset | Expected Output | Evaluation Type |\n",
    "|----|------|--------|-----------------|----------------|\n",
    "| 1 | OE | GSM8K | Integer | Numeric match |\n",
    "| 2 | OE | GSM8K-MC | Letter | Letter match |\n",
    "| 3 | MC | GSM8K-MC | Letter | Letter match |\n",
    "| 4 | MC | GSM8K | Integer | Numeric match |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Design Principles\n",
    "\n",
    "- **Strict output formats**\n",
    "  - No implicit conversions\n",
    "  - No guessing intent\n",
    "- **Cross runs are intentionally harsh**\n",
    "  - They measure format and task transfer\n",
    "- **A correct answer in the wrong format is incorrect**\n",
    "- **Evaluation favors precision over generosity**\n",
    "\n",
    "---\n",
    "\n",
    "## Known Limitations\n",
    "\n",
    "- Cross runs underestimate semantic understanding\n",
    "- Models are penalized for format mismatch\n",
    "- No partial credit\n",
    "- No reasoning-based validation\n",
    "\n",
    "This is intentional and aligned with measuring task specialization vs generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "MODEL_ID = \"Qwen/Qwen2.5-3B\"   # change to your “general model” HF repo id\n",
    "MODEL_DIR = \"/workspace/models/qwen2p5_3b\"  # persistent on your pod\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA required\"\n",
    "device = \"cuda\"\n",
    "\n",
    "# Optional speed knobs on H100\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "# -----------------------\n",
    "# Download / Load tokenizer + model (Flash Attention)\n",
    "# -----------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=MODEL_DIR\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    cache_dir=MODEL_DIR\n",
    ").eval()\n",
    "\n",
    "print(\"Loaded:\", MODEL_ID)\n",
    "print(\"Cache dir:\", MODEL_DIR)\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "\n",
    "# Helper Functions for Answer Extraction\n",
    "\n",
    "def extract_numerical_answer(text: str) -> str:\n",
    "    \"\"\"Extract numerical answer (for open-ended format: #### NUMBER).\"\"\"\n",
    "    # Look for #### pattern\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    \n",
    "    # Fallback: look for last number in text\n",
    "    numbers = re.findall(r'-?\\d+(?:,\\d{3})*(?:\\.\\d+)?', text)\n",
    "    if numbers:\n",
    "        return numbers[-1].replace(',', '')\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def extract_mc_answer(text: str) -> str:\n",
    "    \"\"\"Extract multiple choice answer (A, B, C, D, E).\"\"\"\n",
    "    patterns = [\n",
    "        r'(?:answer is|answer:|Answer is|Answer:)\\s*\\(?([A-E])\\)?',\n",
    "        r'\\(([A-E])\\)',\n",
    "        r'^([A-E])\\.',\n",
    "        r'\\b([A-E])\\s*$',\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE | re.MULTILINE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    \n",
    "    # Fallback\n",
    "    match = re.search(r'\\b([A-E])\\b', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return \"\"\n",
    "\n",
    "def get_ground_truth_gsm8k(answer_text: str) -> str:\n",
    "    \"\"\"Extract ground truth from GSM8K format.\"\"\"\n",
    "    match = re.search(r'####\\s*(-?\\d+(?:,\\d{3})*(?:\\.\\d+)?)', answer_text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return \"\"\n",
    "\n",
    "def get_ground_truth_mc(example: dict) -> str:\n",
    "    \"\"\"Extract ground truth letter from GSM8K-MC.\"\"\"\n",
    "    if 'answer_index' in example:\n",
    "        idx = example['answer_index']\n",
    "    elif 'answer' in example:\n",
    "        idx = example['answer']\n",
    "    else:\n",
    "        return \"\"\n",
    "    return chr(65 + idx)  # 0->A, 1->B, etc.\n",
    "\n",
    "def compare_numerical(pred: str, gold: str) -> bool:\n",
    "    \"\"\"Compare numerical answers.\"\"\"\n",
    "    try:\n",
    "        pred_num = float(pred.replace(',', ''))\n",
    "        gold_num = float(gold.replace(',', ''))\n",
    "        return abs(pred_num - gold_num) < 1e-3\n",
    "    except (ValueError, AttributeError):\n",
    "        return pred.strip() == gold.strip()\n",
    "        \n",
    "# -----------------------\n",
    "# Batched generation helper (batch=16)\n",
    "# -----------------------\n",
    "def generate_response(model, tokenizer, prompts, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    Compatible with previous evals.\n",
    "    Accepts a list of prompts and returns a list of responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure batched input\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    )\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
    "    responses = tokenizer.batch_decode(\n",
    "        outputs[:, prompt_len:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return responses\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATASETS (OFFICIAL SPLITS)\n",
    "# ============================================================\n",
    "\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"test\")\n",
    "gsm8k_mc = load_dataset(\"guipenedo/gsm8k-mc\", split=\"test\")\n",
    "\n",
    "print(f\"✓ GSM8K size: {len(gsm8k)}\")\n",
    "print(f\"✓ GSM8K-MC size: {len(gsm8k_mc)}\")\n",
    "\n",
    "# Evaluation Function: Model on GSM8K (Open-ended) — batched\n",
    "\n",
    "def evaluate_on_gsm8k(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    model_name: str,\n",
    "    num_samples: int = None\n",
    ") -> Dict:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if num_samples is not None:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_size = CONFIG.get(\"batch_size\", 1)\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Evaluating {model_name} on GSM8K (Open-ended)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(dataset, desc=f\"{model_name} → GSM8K\")):\n",
    "        question = example[\"question\"]\n",
    "        ground_truth = get_ground_truth_gsm8k(example[\"answer\"])\n",
    "\n",
    "        prompt = (\n",
    "            \"Solve the following math problem step by step. \"\n",
    "            \"Show your work and put your final answer after ####.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\nAnswer:\"\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, question, ground_truth))\n",
    "\n",
    "        # Run generation when batch is full or at dataset end\n",
    "        if len(batch_prompts) == batch_size or idx == len(dataset) - 1:\n",
    "            responses = generate_response(model, tokenizer, batch_prompts)\n",
    "\n",
    "            for response, (ex_idx, q, gt) in zip(responses, batch_meta):\n",
    "                predicted = extract_numerical_answer(response)\n",
    "                is_correct = compare_numerical(predicted, gt)\n",
    "\n",
    "                correct += int(is_correct)\n",
    "                total += 1\n",
    "\n",
    "                results.append({\n",
    "                    \"index\": ex_idx,\n",
    "                    \"question\": q,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"full_response\": response,\n",
    "                    \"correct\": is_correct\n",
    "                })\n",
    "\n",
    "                if ex_idx < 2:\n",
    "                    print(f\"\\n--- Example {ex_idx + 1} ---\")\n",
    "                    print(f\"Question: {q[:80]}...\")\n",
    "                    print(f\"Ground Truth: {gt}\")\n",
    "                    print(f\"Predicted: {predicted}\")\n",
    "                    print(f\"Correct: {is_correct}\")\n",
    "\n",
    "            batch_prompts.clear()\n",
    "            batch_meta.clear()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n✓ {model_name} on GSM8K: {accuracy:.2%} ({correct}/{total})\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": \"GSM8K\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"results\": results\n",
    "    }\n",
    "# Evaluation Function: Model on GSM8K-MC — batched\n",
    "\n",
    "def evaluate_on_gsm8k_mc(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    dataset,\n",
    "    model_name: str,\n",
    "    num_samples: int = None\n",
    ") -> Dict:\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if num_samples is not None:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    batch_size = CONFIG.get(\"batch_size\", 1)\n",
    "    batch_prompts = []\n",
    "    batch_meta = []\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Evaluating {model_name} on GSM8K-MC\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    for idx, example in enumerate(tqdm(dataset, desc=f\"{model_name} → GSM8K-MC\")):\n",
    "        question = example[\"Question\"]\n",
    "        choices = {\n",
    "            \"A\": example[\"A\"],\n",
    "            \"B\": example[\"B\"],\n",
    "            \"C\": example[\"C\"],\n",
    "            \"D\": example[\"D\"],\n",
    "        }\n",
    "        ground_truth = example[\"Answer\"]\n",
    "\n",
    "        choices_text = \"\\n\".join([f\"{k}. {v}\" for k, v in choices.items()])\n",
    "\n",
    "        prompt = (\n",
    "            \"Answer the following multiple choice question. \"\n",
    "            \"Only provide the letter of the correct answer.\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"{choices_text}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "\n",
    "        batch_prompts.append(prompt)\n",
    "        batch_meta.append((idx, question, choices, ground_truth))\n",
    "\n",
    "        # Run generation when batch is full or at dataset end\n",
    "        if len(batch_prompts) == batch_size or idx == len(dataset) - 1:\n",
    "            responses = generate_response(model, tokenizer, batch_prompts)\n",
    "\n",
    "            for response, (ex_idx, q, ch, gt) in zip(responses, batch_meta):\n",
    "                predicted = extract_mc_answer(response)\n",
    "                is_correct = predicted == gt\n",
    "\n",
    "                correct += int(is_correct)\n",
    "                total += 1\n",
    "\n",
    "                results.append({\n",
    "                    \"index\": ex_idx,\n",
    "                    \"question\": q,\n",
    "                    \"choices\": ch,\n",
    "                    \"ground_truth\": gt,\n",
    "                    \"predicted\": predicted,\n",
    "                    \"full_response\": response,\n",
    "                    \"correct\": is_correct\n",
    "                })\n",
    "\n",
    "                if ex_idx < 2:\n",
    "                    print(f\"\\n--- Example {ex_idx + 1} ---\")\n",
    "                    print(f\"Question: {q[:80]}...\")\n",
    "                    print(f\"Ground Truth: {gt}\")\n",
    "                    print(f\"Predicted: {predicted}\")\n",
    "                    print(f\"Correct: {is_correct}\")\n",
    "\n",
    "            batch_prompts.clear()\n",
    "            batch_meta.clear()\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"\\n✓ {model_name} on GSM8K-MC: {accuracy:.2%} ({correct}/{total})\")\n",
    "\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"dataset\": \"GSM8K-MC\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"correct\": correct,\n",
    "        \"total\": total,\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "# Base model → GSM8K (open-ended)\n",
    "base_on_gsm8k = evaluate_on_gsm8k(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    gsm8k,\n",
    "    model_name=\"Base-Qwen2.5-3B\"\n",
    ")\n",
    "\n",
    "# Base model → GSM8K-MC (multiple choice)\n",
    "base_on_mc = evaluate_on_gsm8k_mc(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    gsm8k_mc,\n",
    "    model_name=\"Base-Qwen2.5-3B\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DETAILED RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Base Model → GSM8K (Open-ended)\")\n",
    "print(f\"   Accuracy: {base_on_gsm8k['accuracy']:.2%} \"\n",
    "      f\"({base_on_gsm8k['correct']}/{base_on_gsm8k['total']})\")\n",
    "\n",
    "print(f\"\\n2. Base Model → GSM8K-MC (Multiple Choice)\")\n",
    "print(f\"   Accuracy: {base_on_mc['accuracy']:.2%} \"\n",
    "      f\"({base_on_mc['correct']}/{base_on_mc['total']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_results = {\n",
    "    \"base_on_gsm8k\": base_on_gsm8k,\n",
    "    \"base_on_gsm8k_mc\": base_on_mc,\n",
    "    \"summary\": {\n",
    "        \"model\": \"Base-Qwen2.5-3B\",\n",
    "        \"gsm8k_accuracy\": base_on_gsm8k[\"accuracy\"],\n",
    "        \"gsm8k_mc_accuracy\": base_on_mc[\"accuracy\"],\n",
    "        \"gsm8k_correct\": base_on_gsm8k[\"correct\"],\n",
    "        \"gsm8k_total\": base_on_gsm8k[\"total\"],\n",
    "        \"gsm8k_mc_correct\": base_on_mc[\"correct\"],\n",
    "        \"gsm8k_mc_total\": base_on_mc[\"total\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results saved to evaluation_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Log scalar metrics\n",
    "wandb.log({\n",
    "    \"gsm8k_accuracy\": base_on_gsm8k[\"accuracy\"],\n",
    "    \"gsm8k_mc_accuracy\": base_on_mc[\"accuracy\"],\n",
    "})\n",
    "\n",
    "# Log summary table\n",
    "results_table = wandb.Table(\n",
    "    columns=[\"Model\", \"Dataset\", \"Accuracy\", \"Correct\", \"Total\"],\n",
    "    data=[\n",
    "        [\n",
    "            \"Base-Qwen2.5-3B\",\n",
    "            \"GSM8K\",\n",
    "            base_on_gsm8k[\"accuracy\"],\n",
    "            base_on_gsm8k[\"correct\"],\n",
    "            base_on_gsm8k[\"total\"],\n",
    "        ],\n",
    "        [\n",
    "            \"Base-Qwen2.5-3B\",\n",
    "            \"GSM8K-MC\",\n",
    "            base_on_mc[\"accuracy\"],\n",
    "            base_on_mc[\"correct\"],\n",
    "            base_on_mc[\"total\"],\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "\n",
    "wandb.log({\"evaluation_table\": results_table})\n",
    "\n",
    "# Save JSON as artifact\n",
    "results_artifact = wandb.Artifact(\n",
    "    name=\"evaluation_results\",\n",
    "    type=\"results\",\n",
    "    description=\"Base-Qwen2.5-3B evaluation on GSM8K and GSM8K-MC\",\n",
    ")\n",
    "\n",
    "results_artifact.add_file(\"evaluation_results.json\")\n",
    "wandb.log_artifact(results_artifact)\n",
    "\n",
    "wandb.finish()\n",
    "print(\"\\n✓ Evaluation complete and results stored in wandb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
